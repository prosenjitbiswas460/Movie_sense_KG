{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrap_data(wiki_link, save_file_path = \"/Prabir/knowledge_graph/data_input/cureus/movie.txt\"):\n",
    "    wikipedia_movie_link = wiki_link\n",
    "    \n",
    "    page_to_scrape = requests.get(wikipedia_movie_link)\n",
    "    soup = BeautifulSoup(page_to_scrape.text, \"html.parser\")\n",
    "    \n",
    "    para = ''\n",
    "    for paragraph in soup.select('p'):\n",
    "        p = paragraph.getText()\n",
    "        para += p\n",
    "\n",
    "\n",
    "     # Open the file in write mode and save the paragraph\n",
    "    with open(save_file_path, 'w') as file:\n",
    "        file.write(para)\n",
    "    \n",
    "    print(f\"Paragraph saved to {save_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_aspect(aspect, content):\n",
    "    introduction = f\"HERE IS THE DETAILS OF MOVIE'S {aspect.upper()}: \\n\"\n",
    "    underline = \"-----------------------------------------------------\\n\"\n",
    "    para = introduction + underline + content\n",
    "    # print(para)\n",
    "    # Open the file in write mode and save the paragraph\n",
    "    with open(save_file_path, 'w') as file:\n",
    "        file.write(para)\n",
    "    \n",
    "    print(f\"Paragraph saved to {save_file_path} for {aspect}.\")\n",
    "\n",
    "\n",
    "def scrape(wiki_link):\n",
    "    Aspects = [\n",
    "        'Plot',\n",
    "        'Cast',\n",
    "        'Production',\n",
    "        'Music',\n",
    "        'Soundtrack',\n",
    "        'Themes',\n",
    "        'Accolades',\n",
    "        ]\n",
    "\n",
    "    Aspects = [aspect.lower() for aspect in Aspects]\n",
    "    \n",
    "    # Send a request to the Wikipedia page\n",
    "    response = requests.get(wiki_link)\n",
    "    \n",
    "    # Parse the page content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    scraped_dict = {}\n",
    "    \n",
    "    #-------------------Summary scraping--------------------------\n",
    "    # Find the first paragraph after the title, which is usually the summary\n",
    "    summary_paragraphs = []\n",
    "    # Wikipedia's summary paragraphs are inside <p> tags but before any <h2> tag\n",
    "    for paragraph in soup.find_all('p'):\n",
    "        \n",
    "        # Ensure the paragraph has text and is not empty\n",
    "        if paragraph.get_text().strip():\n",
    "            summary_paragraphs.append(paragraph.get_text().strip())\n",
    "        \n",
    "        # Stop once we hit the first section heading (e.g. 'Plot' or 'Contents')\n",
    "        if paragraph.find_next_sibling(['h2', 'h3']):\n",
    "            break\n",
    "    \n",
    "    scraped_dict[\"summary\"] =  ' '.join(summary_paragraphs)\n",
    "    \n",
    "    \n",
    "    # -------------------Other Aspect Scraping---------------------\n",
    "    Headings = soup.find_all('div', class_ = 'mw-heading mw-heading2')\n",
    "    for heading in Headings:\n",
    "        aspect, _ = (heading.text).split('[')\n",
    "        if aspect.lower() in Aspects:\n",
    "            next_siblings = heading.find_next_siblings()\n",
    "            text = ''\n",
    "            for next_sibling in next_siblings:\n",
    "                next_sibling_name = next_sibling.name\n",
    "                # sub_sibling = ''\n",
    "                # print(f\"............next_sibling_name = {next_sibling_name}\")\n",
    "                if (next_sibling_name =='style'):\n",
    "                    continue\n",
    "                \n",
    "                elif (next_sibling_name == 'div'):\n",
    "                    clss = next_sibling.get('class')\n",
    "                    \n",
    "                    if ('mw-heading2' in clss):  # break because heading ended\n",
    "                        break\n",
    "                text += \" \"+ next_sibling.text\n",
    "            scraped_dict[aspect] = text \n",
    "    return scraped_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader, UnstructuredPDFLoader, PyPDFium2Loader\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "from helpers.df_helpers import documents2Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input data directory\n",
    "data_dir = \"cureus\"\n",
    "inputdirectory = Path(f\"./data_input/{data_dir}\")\n",
    "## This is where the output csv files will be written\n",
    "out_dir = data_dir\n",
    "outputdirectory = Path(f\"./data_output/{out_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_document(inputdirectory):\n",
    "#     loader = DirectoryLoader(inputdirectory, show_progress=True)\n",
    "#     documents = loader.load()\n",
    "    \n",
    "#     splitter = RecursiveCharacterTextSplitter(\n",
    "#         chunk_size=1500,\n",
    "#         chunk_overlap=150,\n",
    "#         length_function=len,\n",
    "#         is_separator_regex=False,\n",
    "#     )\n",
    "    \n",
    "#     pages = splitter.split_documents(documents)\n",
    "#     # print(\"Number of chunks = \", len(pages))\n",
    "#     # print(pages[3].page_content)\n",
    "\n",
    "#     # Create dataframe of chunks\n",
    "#     df = documents2Dataframe(pages)\n",
    "#     print(\"from load document\")\n",
    "#     print(df.head())\n",
    "#     # df.head()\n",
    "    \n",
    "    # return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain\n",
    "# !pip install -U langchain-community\n",
    "# !pip install unstructured\n",
    "# !sudo apt-get install libmagic1\n",
    "# !pip install yachalk\n",
    "#ollama serve\n",
    "#ollama run zephyr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function uses the helpers/prompt function to extract concepts from text\n",
    "from helpers.df_helpers import df2Graph\n",
    "from helpers.df_helpers import graph2Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def node_generation(df):\n",
    "#     ## To regenerate the graph with LLM, set this to True\n",
    "#     regenerate = True\n",
    "    \n",
    "#     if regenerate:\n",
    "#         concepts_list = df2Graph(df, model='zephyr:latest')\n",
    "#         dfg1 = graph2Df(concepts_list)\n",
    "#         if not os.path.exists(outputdirectory):\n",
    "#             os.makedirs(outputdirectory)\n",
    "        \n",
    "#         dfg1.to_csv(outputdirectory/\"graph.csv\", sep=\"|\", index=False)\n",
    "#         df.to_csv(outputdirectory/\"chunks.csv\", sep=\"|\", index=False)\n",
    "#     else:\n",
    "#         dfg1 = pd.read_csv(outputdirectory/\"graph.csv\", sep=\"|\")\n",
    "    \n",
    "#     dfg1.replace(\"\", np.nan, inplace=True)\n",
    "#     dfg1.dropna(subset=[\"node_1\", \"node_2\", 'edge'], inplace=True)\n",
    "#     dfg1['count'] = 4 \n",
    "#     ## Increasing the weight of the relation to 4. \n",
    "#     ## We will assign the weight of 1 when later the contextual proximity will be calculated. \n",
    "\n",
    "#     print(\"from node generation\")\n",
    "#     print(dfg1.shape)\n",
    "#     print(dfg1.head())\n",
    "\n",
    "#     return dfg1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strat from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"YoR\", \"movie_name\", \"imdb_rating\", \"wiki_link\", \"popular\"]\n",
    "movie_links = pd.read_excel(\"Movie_list.xlsx\", sheet_name = \"hollywood\")\n",
    "movie_links.columns = columns\n",
    "# movie_links.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_movie_links = movie_links[movie_links.popular == \"popular\"]\n",
    "least_popular_movie_links = movie_links[movie_links.popular == \"Least popular\"]\n",
    "# least_popular_movie_links.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_output_folder = \"/Prabir/knowledge_graph/hollywood/\"\n",
    "movie_categories = [least_popular_movie_links, popular_movie_links]\n",
    "\n",
    "for movie_category in movie_categories:\n",
    "    for index, row in movie_category.iterrows():\n",
    "        try:\n",
    "            movie_name = row[\"movie_name\"]\n",
    "            YoR = row[\"YoR\"]\n",
    "            wiki_link = row[\"wiki_link\"]\n",
    "            popular = row[\"popular\"]\n",
    "\n",
    "            # scrape data from the given link aspectwise and get it in a dictionary:\n",
    "            aspect_dict = scrape(wiki_link)\n",
    "            \n",
    "            # Now, generate nodes from each aspect individually after saving their content in the data_input directory one by one.\n",
    "            for aspect in aspect_dict:\n",
    "                save_aspect(aspect, aspect_dict[aspect])\n",
    "\n",
    "                #load document in dataframe chunk\n",
    "                loader = DirectoryLoader(inputdirectory, show_progress=True)\n",
    "                documents = loader.load()\n",
    "                \n",
    "                splitter = RecursiveCharacterTextSplitter(\n",
    "                    chunk_size=1500,\n",
    "                    chunk_overlap=150,\n",
    "                    length_function=len,\n",
    "                    is_separator_regex=False,\n",
    "                )            \n",
    "                pages = splitter.split_documents(documents)\n",
    "\n",
    "                \n",
    "                # Create dataframe of chunks\n",
    "                df = documents2Dataframe(pages)\n",
    "\n",
    "                #node generation task\n",
    "                ## To regenerate the graph with LLM, set this to True\n",
    "                regenerate = True\n",
    "                \n",
    "                if regenerate:\n",
    "                    concepts_list = df2Graph(df, model='zephyr:latest')\n",
    "                    dfg1 = graph2Df(concepts_list)\n",
    "                    if not os.path.exists(outputdirectory):\n",
    "                        os.makedirs(outputdirectory)\n",
    "                    \n",
    "                    dfg1.to_csv(outputdirectory/\"graph.csv\", sep=\"|\", index=False)\n",
    "                    df.to_csv(outputdirectory/\"chunks.csv\", sep=\"|\", index=False)\n",
    "                else:\n",
    "                    dfg1 = pd.read_csv(outputdirectory/\"graph.csv\", sep=\"|\")\n",
    "                \n",
    "                dfg1.replace(\"\", np.nan, inplace=True)\n",
    "                dfg1.dropna(subset=[\"node_1\", \"node_2\", 'edge'], inplace=True)\n",
    "                dfg1['count'] = 4 \n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "                \n",
    "                #save the nodes dataframe in csv_file\n",
    "                save_folder_name = popular\n",
    "                save_file_name = movie_name + \"_\" + str(YoR) + \"_\" + aspect +\".csv\"\n",
    "                save_path = os.path.join(root_output_folder, save_folder_name, save_file_name)      \n",
    "                dfg1.to_csv(save_path, index=False)\n",
    "\n",
    "            except:\n",
    "                continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
